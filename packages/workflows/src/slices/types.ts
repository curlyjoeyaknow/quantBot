/**
 * Pure types for the slice export + analyze workflow.
 * No DB clients, no fs, no env. Just data contracts.
 */

export type Chain = 'sol' | 'eth' | 'base' | 'bsc' | 'unknown';

export type SliceGranularity = 'raw' | '1s' | '1m' | '5m' | '1h' | '1d';

export type Compression = 'zstd' | 'snappy' | 'gzip' | 'none';

export type ParquetPath = string; // adapter decides local path or s3-like uri

export interface RunContext {
  runId: string;
  strategyId?: string;
  seed?: string;
  createdAtIso: string; // deterministic stamp from caller
  note?: string;
}

export interface SliceSpec {
  /**
   * What you're extracting.
   * Example: "trades", "candles_1s", "sim_fills"
   */
  dataset: string;

  chain: Chain;

  /**
   * Bounded slice definition (key to sanity).
   */
  timeRange: { startIso: string; endIso: string };

  /**
   * Optional entity filter.
   * Prefer stable ids like mint/token_id.
   */
  tokenIds?: string[];

  /**
   * Columns to export (keep slices lean).
   * Adapter may enforce allowlist.
   */
  columns?: string[];

  granularity?: SliceGranularity;

  /**
   * Freeform tags useful for partitioning/lineage.
   * Example: { cohort: "top_volume_7d", purpose: "feature_eng" }
   */
  tags?: Record<string, string>;
}

export interface ParquetLayoutSpec {
  /**
   * Example:
   * baseUri: "file:///data/slices" or "s3://quantbot-artifacts"
   * subdirTemplate: "{dataset}/chain={chain}/dt={yyyy}-{mm}-{dd}/run_id={runId}"
   *
   * Adapter expands template deterministically.
   */
  baseUri: string;
  subdirTemplate: string;

  /**
   * Maximum rows per parquet file (adapter may approximate).
   */
  maxRowsPerFile?: number;

  compression?: Compression;

  /**
   * Partition keys are reflected in the path, not necessarily parquet partitioning.
   * Keep it sparse to avoid file explosion.
   */
  partitionKeys?: Array<'dt' | 'chain' | 'dataset' | 'runId' | 'strategyId'>;
}

export interface SliceManifestV1 {
  version: 1;

  manifestId: string; // deterministic id generated by adapter (e.g., hash of spec+ctx+rowCount)
  createdAtIso: string;

  run: RunContext;
  spec: SliceSpec;

  layout: ParquetLayoutSpec;

  /**
   * Concrete artifact locations produced by exporter.
   */
  parquetFiles: Array<{
    path: ParquetPath;
    rowCount?: number;
    byteSize?: number;
    /**
     * Optional: dt partition for sanity checking.
     */
    dt?: string; // YYYY-MM-DD
  }>;

  /**
   * Useful summary for validation + quick UI.
   */
  summary: {
    totalFiles: number;
    totalRows?: number;
    totalBytes?: number;
    /**
     * Adapter can include min/max timestamps discovered during export.
     */
    timeRangeObserved?: { startIso: string; endIso: string };
  };

  /**
   * Optional integrity info.
   */
  integrity?: {
    specHash?: string;
    contentHash?: string;
  };
}

/**
 * Analysis specification.
 * Keep it simple: either SQL text or a named plan id.
 */
export type AnalysisSpec =
  | { kind: 'sql'; sql: string }
  | { kind: 'plan'; planId: string; params?: Record<string, string | number | boolean | null> };

export interface AnalysisResult {
  /**
   * Minimal "what happened" signal.
   */
  status: 'ok' | 'skipped' | 'failed';

  /**
   * A stable, small summary you can put into ClickHouse for dashboards.
   */
  summary?: Record<string, string | number | boolean | null>;

  /**
   * Optional produced artifacts (parquet/csv/json).
   * Adapters decide actual path.
   */
  artifacts?: Array<{
    kind: 'parquet' | 'csv' | 'json';
    path: string;
    description?: string;
  }>;

  /**
   * Any warnings that should bubble up.
   */
  warnings?: string[];
}

export interface ExportAndAnalyzeResult {
  manifest: SliceManifestV1;
  analysis: AnalysisResult;
}
