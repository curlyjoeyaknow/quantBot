---
description: Mandatory regression test creation after bug fixes - prevents bugs from reoccurring
globs:
  - "**/tests/**"
  - "**/*.test.ts"
  - "**/*.test.py"
  - "**/test_*.py"
alwaysApply: true
---

# Debugging and Regression Test Rules

## Core Principle

**After finding and fixing a bug, ALWAYS create tests that would have detected the bug and prevent it from reoccurring.**

This is not optional - it's a mandatory step in the debugging workflow.

## The Bug Fix → Test Workflow

### Step 1: Identify the Bug
- Understand what went wrong
- Identify the root cause
- Document the failure mode

### Step 2: Fix the Bug
- Implement the fix
- Verify the fix works
- Update code as needed

### Step 3: Create Regression Tests (MANDATORY)
- **Write tests that would have caught the bug BEFORE it was fixed**
- Test the specific failure mode
- Test edge cases related to the bug
- Test the fix itself
- Ensure tests fail with the old buggy code (if possible)

### Step 4: Document the Prevention
- Add comments explaining what the test prevents
- Mark critical tests with `CRITICAL:` comments
- Document why the test exists in test docstrings

## Test Types for Regression Prevention

### 1. Direct Failure Mode Tests
Test the exact scenario that caused the bug:

```python
def test_connection_timeout_handled_correctly(self):
    """
    CRITICAL: This test would have caught the original bug.
    
    The original bug used signal handlers which conflicted with the driver.
    This test verifies timeout is handled using driver's built-in parameters.
    """
    # Test the exact failure mode
```

### 2. Static Analysis Tests
For bugs involving code patterns (like signal handlers, missing parameters):

```python
def test_no_signal_handlers_used(self):
    """
    CRITICAL: Verifies that signal.SIGALRM is NOT used.
    The original bug used signal handlers which conflicted with the driver.
    """
    # Read source code and verify pattern doesn't exist
    source_code = inspect.getsource(function)
    assert 'signal.SIGALRM' not in source_code
```

### 3. Integration Tests
For bugs that only appear in real usage:

```python
@pytest.mark.integration
def test_real_connection_timeout(self):
    """
    INTEGRATION TEST: This would have caught the original bug in real usage.
    
    Attempts to connect to a non-existent server and verifies:
    1. Connection fails with a timeout (not hanging forever)
    2. Error message is user-friendly
    3. Timeout happens within reasonable time
    """
    # Test with real network conditions
```

### 4. Property Tests
For bugs involving error handling or data validation:

```python
def test_error_message_always_includes_actionable_info(self):
    """
    Property test: Error messages must always include actionable information.
    
    This ensures error messages are always helpful, not just sometimes.
    """
    # Test multiple error scenarios
    for error_text in error_scenarios:
        # Verify error message quality
```

### 5. Configuration Tests
For bugs involving missing or incorrect configuration:

```python
def test_timeout_parameters_passed_to_driver(self):
    """
    CRITICAL: Verifies that connect_timeout and send_receive_timeout are passed.
    
    This would have caught if timeout parameters were removed.
    """
    # Verify configuration is actually used
```

## Test Naming and Documentation

### Test Names
- Include what the test prevents: `test_no_signal_handlers_used`
- Include the failure mode: `test_connection_timeout_handled_correctly`
- Include the fix verification: `test_timeout_parameters_passed_to_driver`

### Test Docstrings
**MUST include:**
1. What bug this prevents (with `CRITICAL:` if it would have caught the original bug)
2. Why the test exists
3. What would happen if the bug regressed

**Example:**
```python
def test_no_signal_handlers_used(self):
    """
    CRITICAL: This test would have caught the original bug.
    
    Verifies that signal.SIGALRM is NOT used for timeout handling.
    The original bug used signal handlers which conflicted with the driver.
    
    If this test fails, it means signal handlers are being used again,
    which will cause the same timeout handling issues.
    """
```

## Test Organization

### Location
- Place regression tests in the same test file as other tests for the module
- Group related regression tests together
- Mark critical tests clearly

### Markers
Use pytest markers to categorize:
- `@pytest.mark.integration` - For tests requiring real resources
- `@pytest.mark.regression` - For regression tests (optional, but helpful)

### Test File Structure
```python
class TestModuleName:
    """Regular tests"""
    
    def test_normal_behavior(self):
        ...
    
    # Regression tests section
    def test_regression_bug_xyz(self):
        """
        CRITICAL: Prevents regression of bug XYZ.
        ...
        """
    
    def test_regression_bug_abc(self):
        """
        CRITICAL: Prevents regression of bug ABC.
        ...
        """
```

## Verification Checklist

After creating regression tests, verify:

- [ ] Test would have failed with the buggy code (if possible to test)
- [ ] Test passes with the fixed code
- [ ] Test is clearly documented with what it prevents
- [ ] Test covers the specific failure mode
- [ ] Test covers related edge cases
- [ ] Test is fast enough to run in CI
- [ ] Test is marked appropriately (integration, etc.)

## Examples

### Example 1: Missing Error Handling

**Bug:** Connection timeout not handled, script hangs forever.

**Fix:** Add timeout handling with driver's built-in parameters.

**Regression Tests:**
```python
def test_connection_timeout_raises_timeout_error(self):
    """CRITICAL: Prevents hanging on connection timeout."""
    # Test timeout is handled

def test_timeout_parameters_passed_to_driver(self):
    """CRITICAL: Prevents timeout parameters from being removed."""
    # Verify parameters are used

@pytest.mark.integration
def test_real_connection_timeout(self):
    """INTEGRATION: Would have caught the bug in real usage."""
    # Test with real network
```

### Example 2: Code Pattern Bug

**Bug:** Using signal handlers for timeout (conflicts with library).

**Fix:** Use library's built-in timeout parameters.

**Regression Tests:**
```python
def test_no_signal_handlers_used(self):
    """CRITICAL: Prevents using signal handlers for timeout."""
    # Static analysis of source code
    assert 'signal.SIGALRM' not in source_code
    assert 'signal.alarm' not in source_code
```

### Example 3: Error Message Quality

**Bug:** Error messages were confusing and unhelpful.

**Fix:** Improve error messages with actionable information.

**Regression Tests:**
```python
def test_error_message_always_includes_actionable_info(self):
    """Property test: Error messages must always be helpful."""
    # Test multiple error scenarios
    for error_text in error_scenarios:
        # Verify error message quality
```

## Enforcement

### Pre-Commit
- Verify regression tests exist for recent bug fixes
- Check that tests are properly documented

### Code Review
- Require regression tests for all bug fixes
- Verify tests would have caught the bug
- Ensure tests are well-documented

### CI/CD
- Run all regression tests
- Fail build if regression tests fail
- Track test coverage for bug fixes

## Anti-Patterns (Never Do This)

### ❌ Fix bug without tests
```python
# BAD: Fixed the bug but no tests
def get_clickhouse_client():
    # Fixed timeout handling
    ...
    # No tests to prevent regression
```

### ❌ Generic tests that don't catch the bug
```python
# BAD: Test doesn't actually catch the bug
def test_connection_works(self):
    # This passes even with the bug
    ...
```

### ❌ Tests without documentation
```python
# BAD: No explanation of what this prevents
def test_something(self):
    # What bug does this prevent? Unknown.
    ...
```

## Best Practices

1. **Write tests immediately after fixing** - Don't wait
2. **Test the failure mode** - Not just the happy path
3. **Document what it prevents** - Future developers need to know
4. **Use static analysis when possible** - Catch code patterns
5. **Include integration tests** - Catch real-world issues
6. **Mark critical tests** - Make them easy to find
7. **Verify tests would fail** - If possible, test with buggy code

## Related Rules

- See `testing-workflows.mdc` for general testing practices
- See `testing.mdc` for test organization and structure
- See `changelog-enforcement.mdc` for documenting bug fixes

## Summary

**Every bug fix MUST include regression tests that:**
1. Would have caught the bug before it was fixed
2. Prevent the bug from reoccurring
3. Are clearly documented with what they prevent
4. Cover the specific failure mode and related edge cases

This is not optional - it's a mandatory part of the debugging workflow.
