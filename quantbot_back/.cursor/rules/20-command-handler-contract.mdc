--
name: Testing contracts (QuantBot Backtest)
description: Tests must prove architecture guarantees: handler purity, adapter contracts, replayability, and optimization correctness — plus mandatory regression tests.
globs:

- "tests/**"
- "packages/**/src/**"
- "apps/**/src/**"
alwaysApply: true

---

# Testing contracts (QuantBot Backtest)

These tests exist to enforce the core objective:
**learn optimal post-alert trade management policies under explicit downside constraints, per caller** —
without nondeterminism, silent drift, or I/O leakage into handlers.

## Mandatory rule: bugfix ⇒ regression test

If you fix a bug, you MUST add a regression test that would have failed before the fix.

Preferred regression styles (in order):

1) direct failure-mode test (small, deterministic)
2) golden test (synthetic candle stream / call fixture)
3) replay test (recorded adapter outputs)
4) adapter contract test (if bug was boundary/parsing/units)

## Handler unit tests (required)

For every handler (truth, policy execution, optimization):

- in-memory ports only (stubs/fakes implementing `ports/*`)
- no network
- no filesystem
- no real clock (`ClockPort` stub required)
- no implicit randomness (`RandomPort` stub if used)
- no process spawning (use a queue port or return events)

Assert:

- deterministic output (same inputs ⇒ same output)
- emitted `events` (data-only)
- `metrics` shape (structured counters/timings as data)
- structured `warnings` (especially per-call skip/coverage reasons)
- invariants:
  - truth layer: exactly one persisted metrics row per eligible call
  - policy layer: realized return <= peak capture (unless explicit model allows otherwise)
  - optimization layer: selected policy satisfies constraints OR returns “no feasible policy”

## Domain golden tests (required for tricky math)

Golden tests with small synthetic candle streams:

- `computePathMetrics` correctness (ms units, peak multiple, drawdown, dd-to-2x, time-to-2x/3x/4x, alert→activity)
- stop/exit simulation correctness (fixed/time/trailing/ladder/indicator triggers)
- scoring + tie-break logic

Golden tests must be stable and human-reviewable.

## Adapter contract tests (required)

For each adapter implementing a port:

- contract suite per port
- recorded fixtures or mock servers (no live dependencies in CI)
- verify:
  - request formation (SQL/HTTP/RPC)
  - response parsing + normalization (especially timestamp units seconds→ms)
  - ordering determinism (stable sorts)
  - retry/timeout policy (adapter-level)
  - error classification (recoverable vs terminal)

## Replay tests (strongly recommended)

Purpose: prove refactors don’t silently change decision logic.

- same command + recorded adapter outputs ⇒ same handler outputs
- cover:
  - truth layer runs
  - at least one policy evaluation run
  - at least one optimizer run (small parameter grid)

## Integration smoke tests (recommended)

Minimal end-to-end tests for apps:

- CLI smoke: run a tiny path-only job against a fixture DB and confirm completion
- Lab UI smoke: create a run record, trigger execution (mocked), verify endpoints return correct shapes

Composition roots are wiring — smoke test them, don’t unit-test them to death.

## Test naming conventions (recommended)

- `*.handler.test.ts`
- `*.golden.test.ts`
- `*.adapter.contract.test.ts`
- `*.replay.test.ts`
- `*.regression.test.ts` (optional tag when it’s explicitly a “bug came back” guard)
EOF
