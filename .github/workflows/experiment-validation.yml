name: Experiment Validation

# Validate experiment branches with backtest suite (~20 min)
on:
  push:
    branches:
      - 'experiment/**'
  workflow_dispatch:
    inputs:
      upload_artifacts:
        description: 'Upload experiment artifacts'
        required: false
        default: true
        type: boolean

concurrency:
  group: experiment-${{ github.ref }}
  cancel-in-progress: true

jobs:
  validate-backtest:
    name: Validate Backtest Code
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-experiment-${{ hashFiles('tools/backtest/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install pytest duckdb pyarrow pandas numpy scipy

      - name: Run golden tests
        run: |
          cd tools/backtest
          python -m pytest tests/test_path_quality_golden.py -v --tb=long

      - name: Run metric stability tests
        run: |
          cd tools/backtest
          python -m pytest tests/test_existing_metric_stability.py -v --tb=long

      - name: Verify SQL queries parse
        run: |
          cd tools/backtest
          python -c "
          from lib.tp_sl_query import _build_tp_sl_sql
          from lib.baseline_query import _build_baseline_sql
          
          # Test that SQL builds without errors
          sql1 = _build_tp_sl_sql(
              interval_seconds=60,
              horizon_hours=48,
              tp_mult=2.0,
              sl_mult=0.5,
              is_partitioned=False
          )
          print('TP/SL SQL built successfully')
          
          sql2 = _build_baseline_sql(
              interval_seconds=60,
              horizon_hours=48,
              is_partitioned=False
          )
          print('Baseline SQL built successfully')
          "

  validate-optimizer:
    name: Validate Optimizer
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install duckdb pyarrow pandas numpy scipy

      - name: Verify optimizer imports
        run: |
          cd tools/backtest
          python -c "
          from lib.optimizer import GridOptimizer, OptimizationResult, OptimizationRun
          from lib.optimizer_config import OptimizerConfig
          from lib.optimizer_objective import (
              ObjectiveConfig,
              ObjectiveResult,
              score_from_summary,
              QualityFilterConfig,
              compute_retention_bonus,
              compute_giveback_penalty,
              compute_time_quality_penalty,
              compute_headfake_penalty
          )
          from lib.summary import summarize_tp_sl, summarize_baseline, aggregate_by_caller
          
          print('All optimizer modules imported successfully')
          "

      - name: Verify objective config
        run: |
          cd tools/backtest
          python -c "
          from lib.optimizer_objective import ObjectiveConfig, QualityFilterConfig
          
          # Test default config
          cfg = ObjectiveConfig()
          d = cfg.to_dict()
          print(f'ObjectiveConfig fields: {len(d)} fields')
          
          # Verify new path quality fields exist
          required_fields = [
              'retention_bonus_weight',
              'giveback_penalty_weight', 
              'time_quality_penalty_weight',
              'headfake_penalty_weight'
          ]
          for field in required_fields:
              assert field in d, f'Missing field: {field}'
          print('All path quality fields present')
          
          # Test quality filter
          qf = QualityFilterConfig(
              min_retention=0.5,
              max_headfake_rate=0.3,
              max_stall_score=0.4
          )
          print(f'QualityFilterConfig created: {qf}')
          "

  validate-typescript:
    name: Validate TypeScript
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 8

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Type check
        run: pnpm typecheck

      - name: Architecture boundaries
        run: pnpm verify:boundaries-ast

      - name: No live trading
        run: pnpm check:no-live-trading

  run-experiment:
    name: Run Experiment
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [validate-backtest, validate-optimizer]
    if: github.event.inputs.upload_artifacts != 'false'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install duckdb pyarrow pandas numpy scipy tqdm

      - name: Run experiment validation
        run: |
          cd tools/backtest
          echo "Experiment branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          
          # Run full test suite
          python -m pytest tests/ -v --tb=short --ignore=tests/test_duckdb_storage.py 2>&1 | tee experiment-results.txt

      - name: Generate experiment report
        if: always()
        run: |
          cd tools/backtest
          echo "# Experiment Report" > experiment-report.md
          echo "" >> experiment-report.md
          echo "**Branch:** ${{ github.ref_name }}" >> experiment-report.md
          echo "**Commit:** ${{ github.sha }}" >> experiment-report.md
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M UTC')" >> experiment-report.md
          echo "" >> experiment-report.md
          echo "## Test Results" >> experiment-report.md
          echo '```' >> experiment-report.md
          tail -30 experiment-results.txt >> experiment-report.md || true
          echo '```' >> experiment-report.md

      - name: Upload experiment artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: experiment-${{ github.ref_name }}-${{ github.run_id }}
          path: |
            tools/backtest/experiment-results.txt
            tools/backtest/experiment-report.md
            tools/backtest/results/
          retention-days: 30

  experiment-summary:
    name: Experiment Summary
    runs-on: ubuntu-latest
    needs: [validate-backtest, validate-optimizer, validate-typescript, run-experiment]
    if: always()

    steps:
      - name: Generate summary
        run: |
          echo "## Experiment Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Validation | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Backtest Tests | ${{ needs.validate-backtest.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Optimizer | ${{ needs.validate-optimizer.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| TypeScript | ${{ needs.validate-typescript.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run Experiment | ${{ needs.run-experiment.result }} |" >> $GITHUB_STEP_SUMMARY

      - name: Check validation status
        if: needs.validate-backtest.result == 'failure' || needs.validate-optimizer.result == 'failure'
        run: |
          echo "::error::Experiment validation failed!"
          exit 1

